{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment5_softcomputing_mit2020017",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3fNRgdNyqk_"
      },
      "source": [
        "#DATA COLLECTION - train and test data is collected from my mail box to get the feel of how actually the algorithms work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5q9RBxyE-CN"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0xzoQ6vJRG-"
      },
      "source": [
        "##train data is collected from my mail box, here zero index indicates normal messsage and one indicates spam mail "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8jAWSB-QTIA",
        "outputId": "fadea4ce-0496-4001-9ee1-3dd9d2b91f63"
      },
      "source": [
        "import csv\n",
        "import pandas\n",
        "with open('innovators.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"index\", \"mail\"])\n",
        "    writer.writerow([0, \"Today SoC tutorial class\"])\n",
        "    writer.writerow([0, \"New assignment: Providing references of the read‚Ä¶\"])\n",
        "    writer.writerow([0, \"Please accept my request to connect\"])\n",
        "    writer.writerow([0, \"Stipend issue/inquiry for M.Tech. & Dual Degree Student\"])\n",
        "    writer.writerow([0, \"Providing references of the read materials for the C1 component (Unit 1 and 2) - Orientation Camp\"])\n",
        "    writer.writerow([0, \"Invitation to Attend Pragma'21 Conference | Aparoksha'21\"])\n",
        "    writer.writerow([0, \"Invitation to attend the Opening Ceremony of SheHacks 4.0\"])\n",
        "    writer.writerow([0, \"Invitation for Webinar by UBA CELL RGIT on March 13, 2021\"])\n",
        "    writer.writerow([0, \"Infosys Certification Powered by InfyTQ: Registrations Open Now!\"])\n",
        "    writer.writerow([0, \"Art of Living's Beyond Breath- A Free Workshop for IIITA\"])\n",
        "    writer.writerow([0, \"Orientation Camp: C1 - Topics Allotted for Article Writing\"])\n",
        "    writer.writerow([0, \"Tomorrow soft computing class is scheduled at 9 am\"])\n",
        "    writer.writerow([0, \"join in the 2nd meeting of ICAEEC-2021\"])\n",
        "    writer.writerow([0, \"Participate in this equality hackathon and win BIG!\"])\n",
        "    writer.writerow([0, \"A talk on Cervical Cancer Awareness on International Womens Day (8th March ,2021)\"])\n",
        "    writer.writerow([0, \"Regarding the submission of the PFC assignemnt due on 31 Jan\"])\n",
        "    writer.writerow([0, \"Inviting your university students to be part of the prestigious WE program's Cohort 3 - TalentSprint's WE (Women Engineer) program is supported by Google\"])\n",
        "    writer.writerow([0, \"Regarding Postman Workshop on Introduction to APIs\"])\n",
        "    writer.writerow([0, \"Fellowship opportunities for students as Swachhta Saarthi Fellowship under Waste to Wealth Mission of the Govt. of India\"])\n",
        "    writer.writerow([1, \"XXXXXX1217 Unlock your reward now!\"])\n",
        "    writer.writerow([1, \"I need new pants? Ending today: ‚Çπ 219 Sports Pants, ‚Çπ 582 Jeans, ‚Çπ 655 Hiking Pants\"])\n",
        "    writer.writerow([1, \"Step into the future. üí°Check out these smart home gadgets\"])\n",
        "    writer.writerow([1, \"XXXXX1217 We miss your presence. Let's meet soon.\"])\n",
        "    writer.writerow([1, \"Opt for contactless toll payment with FASTag\"])\n",
        "    writer.writerow([1, \"Hoodie Clearance? ‚Çπ 654 Racing Hoodie, ‚Çπ 436 Anime Hoodie, ‚Çπ 436 Gaming Hoodie, ‚Çπ 436 Combat Hoodie\"])\n",
        "    writer.writerow([1, \"Improve GATE Rank with Special Alumni Program. Save Flat Rs 15000 on LIVE Online and Classroom Courses. Check Details Inside\"])\n",
        "    writer.writerow([1, \"Your featured recommendations\"])\n",
        "    writer.writerow([1, \"IRCTC introduces Jyotirlinga & Statue of Unity Rail Tour package\"])\n",
        "    writer.writerow([1, \"See what's NEW in Tops\"])\n",
        "    writer.writerow([1, \"Save More! Shave No More\"])\n",
        "    writer.writerow([1, \"Take great strides to feeling great in the latest jeans now on sale!\"])\n",
        "    writer.writerow([1, \"Pay-day extra earning on the cards! Get 500 Z creditsüí∞ with each referral\"])\n",
        "    writer.writerow([1, \"Long Sleeved Shirts, inside!\"])\n",
        "    writer.writerow([1, \"IRCTC's Air Packages Ex Mumbai\"])\n",
        "    writer.writerow([1, \"The easiest way to book hotels for your next vacation\"])\n",
        "    writer.writerow([1, \"XXXXXX1217 Now enjoy instant payment service using UPI\"])\n",
        "    writer.writerow([1, \"Get complete control of your cheques with InstaBIZ app\"])\n",
        "    writer.writerow([1, \"Which is better, a 40 LPA private job or an 8 LPA government job\"])\n",
        "\n",
        "df = pandas.read_csv('innovators.csv')\n",
        "\n",
        "print(df[6:12])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    index                                               mail\n",
            "6       0  Invitation to attend the Opening Ceremony of S...\n",
            "7       0  Invitation for Webinar by UBA CELL RGIT on Mar...\n",
            "8       0  Infosys Certification Powered by InfyTQ: Regis...\n",
            "9       0  Art of Living's Beyond Breath- A Free Workshop...\n",
            "10      0  Orientation Camp: C1 - Topics Allotted for Art...\n",
            "11      0  Tomorrow soft computing class is scheduled at ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUUcC1BtJukz"
      },
      "source": [
        "##test data is collected from my mail box, here zero index indicates normal messsage and one indicates spam mail "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QfuO3IJcPc4",
        "outputId": "c490a6ea-c3be-4d7e-cd44-f1dc5e9f1ae8"
      },
      "source": [
        "import csv\n",
        "with open('innovator.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"index\", \"mail\"])\n",
        "    writer.writerow([0, \"New announcement: Class will start at 7:55 pm\"])\n",
        "    writer.writerow([0, \"Delegate Invitation:: Export Opportunities for MSMEs, NEDFi House, Guwahati, Assam - 23rd March 2021, 10.30 AM - 01.30 PM\"])\n",
        "    writer.writerow([0, \"SCO Secretariat organizing Competition for best article for college students\"])\n",
        "    writer.writerow([0, \"C1 Project Evaluation Schedule for Students Under Dr. Maity or Dr. Venkatesan\"])\n",
        "    writer.writerow([0, \"Mail your reports and ppt\"])\n",
        "    writer.writerow([0, \"PhD Thesis presubmission Seminar of Ms. Shweta\"])\n",
        "    writer.writerow([0, \"Help spread the word | 2021 Generation Google Scholarship: for women in computer science\"])\n",
        "    writer.writerow([0, \"Invitation for Inaugural session in DST Sponsored FDP Programme on Entrepreneurship Development from March 01st- 12th, 2021 in Online Mode at IIIT Allahabad\"])\n",
        "    writer.writerow([0, \"Invitation for Committee Members | E-Summit IIM Lucknow | Enphilia\"])\n",
        "    writer.writerow([0, \"IEEE membership discount and free offers\"])\n",
        "    writer.writerow([0, \"Exclusive Invite for participants of APROKSHA'21 || CODE KAZE|| 2021 Edition || Placement Opportunity & Cash Rewards || IIIT Allahabad\"])\n",
        "    writer.writerow([0, \"HackVerse 2.0 : Workshop on making your First Desktop Application Using Java and MySQL\"])\n",
        "    writer.writerow([0, \"Techtonic ‚Äì Innovations in Clean Energy‚Äô - 3rd edition of Energy Challenge from Social Alpha\"])\n",
        "    writer.writerow([0, \"Convex Optimization tutorial at 6pm\"])\n",
        "    writer.writerow([1, \"Every man needs these accessories. ‚è∞ Shop the sale.\"])\n",
        "    writer.writerow([1, \"Your Skin & Hair,üíÜ‚Äç‚ôÄÔ∏è Crave Extra Care\"])\n",
        "    writer.writerow([1, \"Where did you get that shirt? 70-90% off all dress shirts and unique polos right now ...üëïüëïüëï\"])\n",
        "    writer.writerow([1, \"Say hello to your new V-Necks\"])\n",
        "    writer.writerow([1, \"Tricky Tawang Adventure Package\"])\n",
        "    writer.writerow([1, \"Slimming and stylish! Don't miss out on pants at 50-80% off!\"])\n",
        "    writer.writerow([1, \"Someone just searched for you on Bing\"])\n",
        "    writer.writerow([1, \"How are decision trees built? | Zolzaya Luvsandorj in Towards Data Science\"])\n",
        "    writer.writerow([1, \"Someone saw Flight Stability And Automa... in an Academia search\"])\n",
        "    writer.writerow([1, \"Avail Zero making charges on diamond jewellery this Republic Dayüòç \"])\n",
        "    writer.writerow([1, \"Best Of Designs & Irresistible Offers - The Recipe Of A Perfect Sundayüëå\"])\n",
        "    writer.writerow([1, \"Still Haven't Decided What To Get Her For Valentine's Day? Tap Tap Tapüíù\"])\n",
        "    writer.writerow([1, \"It's Time To Officially Welcome Spring! \"])\n",
        "    writer.writerow([1, \"Flight Stability And Automatic Control NELSON was your top paper last week\"])\n",
        "    writer.writerow([1, \"Protect your loved ones from future uncertainties! Add nominee to your account\"])\n",
        "df_test = pandas.read_csv('innovator.csv')\n",
        "\n",
        "print(df_test.iloc[5,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PhD Thesis presubmission Seminar of Ms. Shweta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGEcrU7RTAgQ"
      },
      "source": [
        "#Navie Bayes classifier - Multi variate Bernoullis Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwusRPJWov3H",
        "outputId": "4c2a192a-07c8-4092-ff16-cd2eee7acb2f"
      },
      "source": [
        "import pandas\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "def getplanestring (str): \n",
        "    pattern = r'[0-9]'\n",
        "    str_punctuation = re.sub(r'[^\\w]', ' ', str)\n",
        "    str_number = re.sub(pattern, '', str_punctuation)\n",
        "    str_number = str_number.lower()\n",
        "    return str_number\n",
        "\n",
        "\n",
        "def get_totalwords(df): \n",
        "    result_dict =[]\n",
        "    for i in range(df.shape[0]): \n",
        "        #print(df.iloc[i,1])\n",
        "        s= getplanestring(df.iloc[i,1])\n",
        "        #print(s)\n",
        "        result_instance = s.split()\n",
        "        result_dict = [*result_dict, *result_instance]\n",
        "    return result_dict\n",
        "\n",
        "def get_uniquewords(given_list): \n",
        "    unique_list = list(dict.fromkeys(given_list))\n",
        "    return unique_list\n",
        "\n",
        "\n",
        "def get_model_vect(total_words, unique_words): \n",
        "    vect = [1]*len(unique_words)\n",
        "    k =0\n",
        "    for i in unique_words: \n",
        "        c = 0\n",
        "        for j in total_words: \n",
        "            if (i == j): \n",
        "                c = c + 1\n",
        "        vect[k] = vect[k] + c\n",
        "        k = k +1 \n",
        "    for i in range(len(unique_words)): \n",
        "        vect[i] = float (vect[i]/len(total_words))\n",
        "    return vect     \n",
        "\n",
        "\n",
        "def get_test_vect(test_words, unique_words): \n",
        "    vect = [0]*len(unique_words)\n",
        "    k =0\n",
        "    for i in unique_words:\n",
        "        for j in test_words: \n",
        "            if (i == j): \n",
        "                vect[k] = 1\n",
        "        k = k +1 \n",
        "    return vect     \n",
        "\n",
        "\n",
        "\n",
        "def get_result(vect, vector0, vector1, phi_0, phi_1): \n",
        "    p_0 = 1\n",
        "    p_1 = 1\n",
        "    for i in range(len(vect)): \n",
        "        if (vect[i]==1): \n",
        "            p_0 = p_0 * vector0[i] * phi_0\n",
        "            p_1 = p_1 * vector1[i] * phi_1\n",
        "\n",
        "    p_class0 = float(p_0/(p_0 + p_1))\n",
        "    p_class1 = float(p_1/(p_1 + p_0))\n",
        "    if (p_class0 > p_class1): \n",
        "        print ('class 0 probability = ' + str(p_class0) +  '   class 1 probability = ' + str(p_class1) + '  predicted calss 0 - normal' )\n",
        "        return 0 \n",
        "    else : \n",
        "        print ('class 0 probability = ' + str(p_class0) +  '   class 1 probability = ' + str(p_class1) + '  predicted calss 1 - spam' )\n",
        "        return 1 \n",
        "\n",
        "\n",
        "def accuracy (df_test, vector0, vector1, phi_0, phi_1, unique_words): \n",
        "    c =0 \n",
        "    result = []\n",
        "    for i in range(df_test.shape[0]): \n",
        "        get_str = df_test.iloc[i,1]\n",
        "        get_plane_str = getplanestring(get_str)\n",
        "        test_words = get_plane_str.split()\n",
        "        vect = get_test_vect(test_words, unique_words)\n",
        "        r = get_result(vect, vector0, vector1, phi_0, phi_1)\n",
        "        result.append(r)\n",
        "    print(result)\n",
        "    for i in range(df_test.shape[0]):\n",
        "        if result[i] == df_test.iloc[i,0]: \n",
        "            c = c+1 \n",
        "    print('final accuracy ratio = ' + str(float( c/ df_test.shape[0])))\n",
        "    return float( c/ df_test.shape[0] )\n",
        "\n",
        "\n",
        "\n",
        "def multivarient_model (df,index): \n",
        "    total_words = get_totalwords(df)\n",
        "    unique_words = get_uniquewords(total_words)\n",
        "    total_words_class0 = get_totalwords(df[0:index])\n",
        "    total_words_class1= get_totalwords(df[index:df.shape[0]])\n",
        "    vector_class0 = get_model_vect (total_words_class0, unique_words)\n",
        "    vector_class1 = get_model_vect (total_words_class1, unique_words)\n",
        "    return vector_class0, vector_class1, unique_words\n",
        "\n",
        "\n",
        "\n",
        "vector0, vector1, unique_words = multivarient_model (df,19)\n",
        "print('bag of all word in training set '+ str(unique_words))\n",
        "print('probabability of words belonging to class 0 ' + str(vector0))\n",
        "print('probabability of words belonging to class 1 ' + str(vector1))\n",
        "accuracy (df_test, vector0, vector1, 0.5, 0.5, unique_words)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bag of all word in training set ['today', 'soc', 'tutorial', 'class', 'new', 'assignment', 'providing', 'references', 'of', 'the', 'read', 'please', 'accept', 'my', 'request', 'to', 'connect', 'stipend', 'issue', 'inquiry', 'for', 'm', 'tech', 'dual', 'degree', 'student', 'materials', 'c', 'component', 'unit', 'and', 'orientation', 'camp', 'invitation', 'attend', 'pragma', 'conference', 'aparoksha', 'opening', 'ceremony', 'shehacks', 'webinar', 'by', 'uba', 'cell', 'rgit', 'on', 'march', 'infosys', 'certification', 'powered', 'infytq', 'registrations', 'open', 'now', 'art', 'living', 's', 'beyond', 'breath', 'a', 'free', 'workshop', 'iiita', 'topics', 'allotted', 'article', 'writing', 'tomorrow', 'soft', 'computing', 'is', 'scheduled', 'at', 'am', 'join', 'in', 'nd', 'meeting', 'icaeec', 'participate', 'this', 'equality', 'hackathon', 'win', 'big', 'talk', 'cervical', 'cancer', 'awareness', 'international', 'womens', 'day', 'th', 'regarding', 'submission', 'pfc', 'assignemnt', 'due', 'jan', 'inviting', 'your', 'university', 'students', 'be', 'part', 'prestigious', 'we', 'program', 'cohort', 'talentsprint', 'women', 'engineer', 'supported', 'google', 'postman', 'introduction', 'apis', 'fellowship', 'opportunities', 'as', 'swachhta', 'saarthi', 'under', 'waste', 'wealth', 'mission', 'govt', 'india', 'xxxxxx', 'unlock', 'reward', 'i', 'need', 'pants', 'ending', 'sports', 'jeans', 'hiking', 'step', 'into', 'future', 'check', 'out', 'these', 'smart', 'home', 'gadgets', 'xxxxx', 'miss', 'presence', 'let', 'meet', 'soon', 'opt', 'contactless', 'toll', 'payment', 'with', 'fastag', 'hoodie', 'clearance', 'racing', 'anime', 'gaming', 'combat', 'improve', 'gate', 'rank', 'special', 'alumni', 'save', 'flat', 'rs', 'live', 'online', 'classroom', 'courses', 'details', 'inside', 'featured', 'recommendations', 'irctc', 'introduces', 'jyotirlinga', 'statue', 'unity', 'rail', 'tour', 'package', 'see', 'what', 'tops', 'more', 'shave', 'no', 'take', 'great', 'strides', 'feeling', 'latest', 'sale', 'pay', 'extra', 'earning', 'cards', 'get', 'z', 'credits', 'each', 'referral', 'long', 'sleeved', 'shirts', 'air', 'packages', 'ex', 'mumbai', 'easiest', 'way', 'book', 'hotels', 'next', 'vacation', 'enjoy', 'instant', 'service', 'using', 'upi', 'complete', 'control', 'cheques', 'instabiz', 'app', 'which', 'better', 'lpa', 'private', 'job', 'or', 'an', 'government']\n",
            "probabability of words belonging to class 0 [0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.016304347826086956, 0.010869565217391304, 0.010869565217391304, 0.016304347826086956, 0.016304347826086956, 0.05434782608695652, 0.05434782608695652, 0.016304347826086956, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.03804347826086957, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.03804347826086957, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.016304347826086956, 0.010869565217391304, 0.010869565217391304, 0.016304347826086956, 0.016304347826086956, 0.016304347826086956, 0.021739130434782608, 0.016304347826086956, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.021739130434782608, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.03260869565217391, 0.016304347826086956, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.021739130434782608, 0.010869565217391304, 0.010869565217391304, 0.016304347826086956, 0.010869565217391304, 0.016304347826086956, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.016304347826086956, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.016304347826086956, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.016304347826086956, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.016304347826086956, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.016304347826086956, 0.016304347826086956, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.016304347826086956, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.010869565217391304, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652, 0.005434782608695652]\n",
            "probabability of words belonging to class 1 [0.011834319526627219, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.01775147928994083, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.01775147928994083, 0.029585798816568046, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.01775147928994083, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.01775147928994083, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.011834319526627219, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.023668639053254437, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.023668639053254437, 0.005917159763313609, 0.005917159763313609, 0.023668639053254437, 0.005917159763313609, 0.005917159763313609, 0.011834319526627219, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.011834319526627219, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.01775147928994083, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.011834319526627219, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.03550295857988166, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.011834319526627219, 0.011834319526627219, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.005917159763313609, 0.01775147928994083, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.023668639053254437, 0.011834319526627219, 0.011834319526627219, 0.01775147928994083, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.01775147928994083, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.01775147928994083, 0.029585798816568046, 0.011834319526627219, 0.03550295857988166, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.01775147928994083, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.01775147928994083, 0.011834319526627219, 0.011834319526627219, 0.01775147928994083, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.01775147928994083, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.01775147928994083, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.01775147928994083, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219, 0.01775147928994083, 0.011834319526627219, 0.01775147928994083, 0.011834319526627219, 0.011834319526627219, 0.011834319526627219]\n",
            "class 0 probability = 0.7560571944578673   class 1 probability = 0.24394280554213266  predicted calss 0 - normal\n",
            "class 0 probability = 0.9865244829743856   class 1 probability = 0.013475517025614345  predicted calss 0 - normal\n",
            "class 0 probability = 0.9155949007673345   class 1 probability = 0.0844050992326654  predicted calss 0 - normal\n",
            "class 0 probability = 0.9320957692086632   class 1 probability = 0.06790423079133691  predicted calss 0 - normal\n",
            "class 0 probability = 0.2966667705379494   class 1 probability = 0.7033332294620506  predicted calss 1 - spam\n",
            "class 0 probability = 0.7537912578055308   class 1 probability = 0.24620874219446923  predicted calss 0 - normal\n",
            "class 0 probability = 0.9242507025815192   class 1 probability = 0.07574929741848076  predicted calss 0 - normal\n",
            "class 0 probability = 0.977034311150426   class 1 probability = 0.022965688849573864  predicted calss 0 - normal\n",
            "class 0 probability = 0.8873064410901876   class 1 probability = 0.11269355890981231  predicted calss 0 - normal\n",
            "class 0 probability = 0.7167786245493102   class 1 probability = 0.28322137545068976  predicted calss 0 - normal\n",
            "class 0 probability = 0.8677485075786446   class 1 probability = 0.13225149242135545  predicted calss 0 - normal\n",
            "class 0 probability = 0.4237489641252561   class 1 probability = 0.5762510358747439  predicted calss 1 - spam\n",
            "class 0 probability = 0.7376710453589821   class 1 probability = 0.2623289546410178  predicted calss 0 - normal\n",
            "class 0 probability = 0.7713977042538825   class 1 probability = 0.2286022957461175  predicted calss 0 - normal\n",
            "class 0 probability = 0.2792352250402743   class 1 probability = 0.7207647749597257  predicted calss 1 - spam\n",
            "class 0 probability = 0.12326875186126708   class 1 probability = 0.8767312481387329  predicted calss 1 - spam\n",
            "class 0 probability = 0.08169102897085022   class 1 probability = 0.9183089710291498  predicted calss 1 - spam\n",
            "class 0 probability = 0.286613104679372   class 1 probability = 0.7133868953206279  predicted calss 1 - spam\n",
            "class 0 probability = 0.3147113594040968   class 1 probability = 0.6852886405959031  predicted calss 1 - spam\n",
            "class 0 probability = 0.14445958155993235   class 1 probability = 0.8555404184400677  predicted calss 1 - spam\n",
            "class 0 probability = 0.7470024921629509   class 1 probability = 0.2529975078370491  predicted calss 0 - normal\n",
            "class 0 probability = 0.47875354107648727   class 1 probability = 0.5212464589235128  predicted calss 1 - spam\n",
            "class 0 probability = 0.3675380522017076   class 1 probability = 0.6324619477982925  predicted calss 1 - spam\n",
            "class 0 probability = 0.6992020881189802   class 1 probability = 0.3007979118810198  predicted calss 0 - normal\n",
            "class 0 probability = 0.8856921279864208   class 1 probability = 0.11430787201357917  predicted calss 0 - normal\n",
            "class 0 probability = 0.35265571595969697   class 1 probability = 0.647344284040303  predicted calss 1 - spam\n",
            "class 0 probability = 0.6631187913564073   class 1 probability = 0.3368812086435928  predicted calss 0 - normal\n",
            "class 0 probability = 0.16227390815040935   class 1 probability = 0.8377260918495907  predicted calss 1 - spam\n",
            "class 0 probability = 0.2315512329516837   class 1 probability = 0.7684487670483164  predicted calss 1 - spam\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1]\n",
            "final accuracy ratio = 0.7931034482758621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7931034482758621"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBIzI-w9rx89"
      },
      "source": [
        "#Navie Bayes classifier - Multinomial Event Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45mugoTGrydC",
        "outputId": "7b9ce3a8-9a90-4cde-ec32-3b4ed1e7198e"
      },
      "source": [
        "from functools import reduce \n",
        "\n",
        "total_words_class0 = get_totalwords(df[0:18])\n",
        "total_words_class1=  get_totalwords(df[18:df.shape[0]])\n",
        "\n",
        "def feature_vector (test_sentence,df): \n",
        "    total_words_class0 = get_totalwords(df[0:18])\n",
        "    total_words_class1=  get_totalwords(df[18:df.shape[0]])\n",
        "    plane_string = getplanestring (test_sentence)\n",
        "    result_instance = plane_string.split()\n",
        "    unique_list = list(dict.fromkeys(result_instance))\n",
        "    #print(unique_list)\n",
        "    feature_vect0 = []\n",
        "    feature_vect1 = []\n",
        "    for i in unique_list: \n",
        "        c = 1\n",
        "        for j in total_words_class0: \n",
        "            if i == j: \n",
        "                c = c+1 \n",
        "        feature_vect0.append(c)\n",
        "    for i in unique_list: \n",
        "        c = 1\n",
        "        for j in total_words_class1: \n",
        "            if i == j: \n",
        "                c = c+1 \n",
        "        feature_vect1.append(c)\n",
        "    for i in range(len(unique_list)): \n",
        "        feature_vect1[i] = float (feature_vect1[i]/len(total_words_class1))\n",
        "\n",
        "    for i in range(len(unique_list)): \n",
        "        feature_vect0[i] = float (feature_vect0[i]/len(total_words_class0))\n",
        "    #print(feature_vect0)\n",
        "    #print(feature_vect1)\n",
        "\n",
        "    return feature_vect0, feature_vect1\n",
        "\n",
        "\n",
        "def result(test_sentence,df, phi0, phi1): \n",
        "    feature0, feature1 = feature_vector (test_sentence,df)\n",
        "    p_0 = reduce(lambda a,b : a*b , feature0)\n",
        "    p_1 = reduce(lambda a,b : a*b , feature1)\n",
        "    probability0 = p_0 * phi0\n",
        "    probability1 = p_1 * phi1\n",
        "    #print('probability of class 0 = ' + str(probability0) + '  probability of class 1 = ' + str(probability1))\n",
        "    if probability0 > probability1 :\n",
        "        return 0 \n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "\n",
        "def accuracy_multinomial (df_test, df, phi0, phi1): \n",
        "    result_vect = []\n",
        "    for i in range(df_test.shape[0]): \n",
        "        r = result(df_test.iloc[i,1], df, phi0, phi1)\n",
        "        result_vect.append(r)\n",
        "    print(result_vect)\n",
        "    c =0\n",
        "    for i in range(df_test.shape[0]):\n",
        "        if result_vect[i] == df_test.iloc[i,0]: \n",
        "            c = c+1 \n",
        "    print('final accuracy ratio = ' + str(float( c/ df_test.shape[0])))\n",
        "    return float( c/ df_test.shape[0] )\n",
        "\n",
        "accuracy_multinomial(df_test, df, 0.5, 0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "final accuracy ratio = 0.6551724137931034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6551724137931034"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYJYuo6MKiJJ"
      },
      "source": [
        "#Gaussian Discriminant Analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hBRaWOZN91e"
      },
      "source": [
        "## converting data to matrix form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-XgC8S9YKtPM",
        "outputId": "59946eef-f11f-4adb-ac2b-e9043680f8cd"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_test_vect1(test_words, unique_words): \n",
        "    vect = [1]*len(unique_words)\n",
        "    k =0\n",
        "    for i in unique_words:\n",
        "        c =1\n",
        "        for j in test_words: \n",
        "            if (i == j): \n",
        "                c = c+1\n",
        "                vect[k] = c\n",
        "        k = k +1 \n",
        "    return vect     \n",
        "\n",
        "\n",
        "def get_train_x (df): \n",
        "    words = get_totalwords(df)\n",
        "    unique_words = get_uniquewords(words)\n",
        "    print(words)\n",
        "    #print(unique_words)\n",
        "    d = np.zeros((1, len(unique_words)))\n",
        "    #print(d)\n",
        "    for i in range(df.shape[0]):\n",
        "        get_str = df.iloc[i,1]\n",
        "        get_plane_str = getplanestring(get_str)\n",
        "        test_words = get_plane_str.split()\n",
        "        print(test_words)\n",
        "        vect = get_test_vect1(test_words, unique_words)\n",
        "        print(vect)\n",
        "        d = np.vstack((d,vect))\n",
        "    d = np.delete(d, (0), axis=0)\n",
        "    return d\n",
        "\n",
        "\n",
        "def get_test_x (df, df_test): \n",
        "    words = get_totalwords(df)\n",
        "    unique_words = get_uniquewords(words)\n",
        "    #print(unique_words)\n",
        "    d = np.zeros((1, len(unique_words)))\n",
        "    #print(d)\n",
        "    for i in range(df_test.shape[0]):\n",
        "        get_str = df_test.iloc[i,1]\n",
        "        get_plane_str = getplanestring(get_str)\n",
        "        test_words = get_plane_str.split()\n",
        "        vect = get_test_vect1(test_words, unique_words)\n",
        "        #print(vect)\n",
        "        d = np.vstack((d,vect))\n",
        "    d = np.delete(d, (0), axis=0)\n",
        "    return d\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_train_y(df):\n",
        "    d = np.zeros((1, 1))\n",
        "    print(d)\n",
        "    for i in range(df.shape[0]):\n",
        "        v = []\n",
        "        v.append(df.iloc[i,0])\n",
        "        d = np.vstack((d,v))\n",
        "    d = np.delete(d, (0), axis=0)\n",
        "    return d\n",
        "\n",
        "\n",
        "X = get_train_x (df)\n",
        "print(X)\n",
        "Y = get_train_y(df)\n",
        "#print(Y)\n",
        "\n",
        "\n",
        "'''\n",
        "X_test = get_test_x(df, df_test)\n",
        "print(X_test)\n",
        "Y_test= get_train_y(df_test)\n",
        "print(Y_test)\n",
        "\n",
        "\n",
        "for row in X_test: \n",
        "  print(row)\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['today', 'soc', 'tutorial', 'class', 'new', 'assignment', 'providing', 'references', 'of', 'the', 'read', 'please', 'accept', 'my', 'request', 'to', 'connect', 'stipend', 'issue', 'inquiry', 'for', 'm', 'tech', 'dual', 'degree', 'student', 'providing', 'references', 'of', 'the', 'read', 'materials', 'for', 'the', 'c', 'component', 'unit', 'and', 'orientation', 'camp', 'invitation', 'to', 'attend', 'pragma', 'conference', 'aparoksha', 'invitation', 'to', 'attend', 'the', 'opening', 'ceremony', 'of', 'shehacks', 'invitation', 'for', 'webinar', 'by', 'uba', 'cell', 'rgit', 'on', 'march', 'infosys', 'certification', 'powered', 'by', 'infytq', 'registrations', 'open', 'now', 'art', 'of', 'living', 's', 'beyond', 'breath', 'a', 'free', 'workshop', 'for', 'iiita', 'orientation', 'camp', 'c', 'topics', 'allotted', 'for', 'article', 'writing', 'tomorrow', 'soft', 'computing', 'class', 'is', 'scheduled', 'at', 'am', 'join', 'in', 'the', 'nd', 'meeting', 'of', 'icaeec', 'participate', 'in', 'this', 'equality', 'hackathon', 'and', 'win', 'big', 'a', 'talk', 'on', 'cervical', 'cancer', 'awareness', 'on', 'international', 'womens', 'day', 'th', 'march', 'regarding', 'the', 'submission', 'of', 'the', 'pfc', 'assignemnt', 'due', 'on', 'jan', 'inviting', 'your', 'university', 'students', 'to', 'be', 'part', 'of', 'the', 'prestigious', 'we', 'program', 's', 'cohort', 'talentsprint', 's', 'we', 'women', 'engineer', 'program', 'is', 'supported', 'by', 'google', 'regarding', 'postman', 'workshop', 'on', 'introduction', 'to', 'apis', 'fellowship', 'opportunities', 'for', 'students', 'as', 'swachhta', 'saarthi', 'fellowship', 'under', 'waste', 'to', 'wealth', 'mission', 'of', 'the', 'govt', 'of', 'india', 'xxxxxx', 'unlock', 'your', 'reward', 'now', 'i', 'need', 'new', 'pants', 'ending', 'today', 'sports', 'pants', 'jeans', 'hiking', 'pants', 'step', 'into', 'the', 'future', 'check', 'out', 'these', 'smart', 'home', 'gadgets', 'xxxxx', 'we', 'miss', 'your', 'presence', 'let', 's', 'meet', 'soon', 'opt', 'for', 'contactless', 'toll', 'payment', 'with', 'fastag', 'hoodie', 'clearance', 'racing', 'hoodie', 'anime', 'hoodie', 'gaming', 'hoodie', 'combat', 'hoodie', 'improve', 'gate', 'rank', 'with', 'special', 'alumni', 'program', 'save', 'flat', 'rs', 'on', 'live', 'online', 'and', 'classroom', 'courses', 'check', 'details', 'inside', 'your', 'featured', 'recommendations', 'irctc', 'introduces', 'jyotirlinga', 'statue', 'of', 'unity', 'rail', 'tour', 'package', 'see', 'what', 's', 'new', 'in', 'tops', 'save', 'more', 'shave', 'no', 'more', 'take', 'great', 'strides', 'to', 'feeling', 'great', 'in', 'the', 'latest', 'jeans', 'now', 'on', 'sale', 'pay', 'day', 'extra', 'earning', 'on', 'the', 'cards', 'get', 'z', 'credits', 'with', 'each', 'referral', 'long', 'sleeved', 'shirts', 'inside', 'irctc', 's', 'air', 'packages', 'ex', 'mumbai', 'the', 'easiest', 'way', 'to', 'book', 'hotels', 'for', 'your', 'next', 'vacation', 'xxxxxx', 'now', 'enjoy', 'instant', 'payment', 'service', 'using', 'upi', 'get', 'complete', 'control', 'of', 'your', 'cheques', 'with', 'instabiz', 'app', 'which', 'is', 'better', 'a', 'lpa', 'private', 'job', 'or', 'an', 'lpa', 'government', 'job']\n",
            "['today', 'soc', 'tutorial', 'class']\n",
            "[2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['new', 'assignment', 'providing', 'references', 'of', 'the', 'read']\n",
            "[1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['please', 'accept', 'my', 'request', 'to', 'connect']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['stipend', 'issue', 'inquiry', 'for', 'm', 'tech', 'dual', 'degree', 'student']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['providing', 'references', 'of', 'the', 'read', 'materials', 'for', 'the', 'c', 'component', 'unit', 'and', 'orientation', 'camp']\n",
            "[1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['invitation', 'to', 'attend', 'pragma', 'conference', 'aparoksha']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['invitation', 'to', 'attend', 'the', 'opening', 'ceremony', 'of', 'shehacks']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['invitation', 'for', 'webinar', 'by', 'uba', 'cell', 'rgit', 'on', 'march']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['infosys', 'certification', 'powered', 'by', 'infytq', 'registrations', 'open', 'now']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['art', 'of', 'living', 's', 'beyond', 'breath', 'a', 'free', 'workshop', 'for', 'iiita']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['orientation', 'camp', 'c', 'topics', 'allotted', 'for', 'article', 'writing']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['tomorrow', 'soft', 'computing', 'class', 'is', 'scheduled', 'at', 'am']\n",
            "[1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['join', 'in', 'the', 'nd', 'meeting', 'of', 'icaeec']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['participate', 'in', 'this', 'equality', 'hackathon', 'and', 'win', 'big']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['a', 'talk', 'on', 'cervical', 'cancer', 'awareness', 'on', 'international', 'womens', 'day', 'th', 'march']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['regarding', 'the', 'submission', 'of', 'the', 'pfc', 'assignemnt', 'due', 'on', 'jan']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['inviting', 'your', 'university', 'students', 'to', 'be', 'part', 'of', 'the', 'prestigious', 'we', 'program', 's', 'cohort', 'talentsprint', 's', 'we', 'women', 'engineer', 'program', 'is', 'supported', 'by', 'google']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['regarding', 'postman', 'workshop', 'on', 'introduction', 'to', 'apis']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['fellowship', 'opportunities', 'for', 'students', 'as', 'swachhta', 'saarthi', 'fellowship', 'under', 'waste', 'to', 'wealth', 'mission', 'of', 'the', 'govt', 'of', 'india']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['xxxxxx', 'unlock', 'your', 'reward', 'now']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['i', 'need', 'new', 'pants', 'ending', 'today', 'sports', 'pants', 'jeans', 'hiking', 'pants']\n",
            "[2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 4, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['step', 'into', 'the', 'future', 'check', 'out', 'these', 'smart', 'home', 'gadgets']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['xxxxx', 'we', 'miss', 'your', 'presence', 'let', 's', 'meet', 'soon']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['opt', 'for', 'contactless', 'toll', 'payment', 'with', 'fastag']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['hoodie', 'clearance', 'racing', 'hoodie', 'anime', 'hoodie', 'gaming', 'hoodie', 'combat', 'hoodie']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['improve', 'gate', 'rank', 'with', 'special', 'alumni', 'program', 'save', 'flat', 'rs', 'on', 'live', 'online', 'and', 'classroom', 'courses', 'check', 'details', 'inside']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['your', 'featured', 'recommendations']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['irctc', 'introduces', 'jyotirlinga', 'statue', 'of', 'unity', 'rail', 'tour', 'package']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['see', 'what', 's', 'new', 'in', 'tops']\n",
            "[1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['save', 'more', 'shave', 'no', 'more']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['take', 'great', 'strides', 'to', 'feeling', 'great', 'in', 'the', 'latest', 'jeans', 'now', 'on', 'sale']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['pay', 'day', 'extra', 'earning', 'on', 'the', 'cards', 'get', 'z', 'credits', 'with', 'each', 'referral']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['long', 'sleeved', 'shirts', 'inside']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['irctc', 's', 'air', 'packages', 'ex', 'mumbai']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['the', 'easiest', 'way', 'to', 'book', 'hotels', 'for', 'your', 'next', 'vacation']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['xxxxxx', 'now', 'enjoy', 'instant', 'payment', 'service', 'using', 'upi']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['get', 'complete', 'control', 'of', 'your', 'cheques', 'with', 'instabiz', 'app']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['which', 'is', 'better', 'a', 'lpa', 'private', 'job', 'or', 'an', 'lpa', 'government', 'job']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 2, 3, 2, 2, 2]\n",
            "[[2. 2. 2. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " ...\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 2. 2. 2.]]\n",
            "[[0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nX_test = get_test_x(df, df_test)\\nprint(X_test)\\nY_test= get_train_y(df_test)\\nprint(Y_test)\\n\\n\\nfor row in X_test: \\n  print(row)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msUWW4LNY1iQ"
      },
      "source": [
        "##calculate mu "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoVVPrglY9Ns",
        "outputId": "e31493c1-1917-44bc-e7ff-30ca170f0edd"
      },
      "source": [
        "def mui(X, Y, index): \n",
        "    m = len(Y)\n",
        "    n = X.shape[1]\n",
        "    sum = [0]*n\n",
        "    c =0\n",
        "\n",
        "    for i in range(m): \n",
        "        if Y[i] == index: \n",
        "            sum = sum + X[i]\n",
        "            c = c+1\n",
        "    #print(c)\n",
        "    return sum/c\n",
        "\n",
        "print(mui(X,Y, 0))\n",
        "print(mui(X,Y, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.05263158 1.05263158 1.05263158 1.10526316 1.05263158 1.05263158\n",
            " 1.10526316 1.10526316 1.47368421 1.47368421 1.10526316 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.31578947 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.31578947 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.10526316 1.05263158 1.05263158\n",
            " 1.10526316 1.10526316 1.10526316 1.15789474 1.10526316 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.15789474 1.05263158 1.05263158 1.05263158 1.26315789 1.10526316\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.15789474 1.05263158 1.05263158\n",
            " 1.10526316 1.05263158 1.10526316 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.10526316\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.10526316 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.10526316 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.10526316 1.05263158 1.05263158 1.05263158 1.10526316\n",
            " 1.10526316 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.10526316 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.        ]\n",
            "[1.05263158 1.         1.         1.         1.10526316 1.\n",
            " 1.         1.         1.10526316 1.21052632 1.         1.\n",
            " 1.         1.         1.         1.10526316 1.         1.\n",
            " 1.         1.         1.10526316 1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.05263158 1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.15789474 1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.15789474 1.         1.         1.15789474 1.         1.\n",
            " 1.05263158 1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.05263158\n",
            " 1.         1.         1.         1.         1.10526316 1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.05263158 1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.26315789\n",
            " 1.         1.         1.         1.         1.         1.05263158\n",
            " 1.05263158 1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.10526316 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.15789474 1.05263158 1.05263158 1.10526316\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.10526316 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.10526316 1.21052632 1.05263158 1.26315789 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.10526316 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.10526316\n",
            " 1.05263158 1.05263158 1.10526316 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.10526316 1.05263158 1.05263158 1.05263158 1.10526316\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.10526316 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158 1.05263158\n",
            " 1.05263158 1.05263158 1.10526316 1.05263158 1.10526316 1.05263158\n",
            " 1.05263158 1.05263158]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0c78LzDZFM5"
      },
      "source": [
        "##calculate sigma "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "PFYAqK7cZGVK",
        "outputId": "f92b2f12-446f-414a-9404-8c578e9e05f7"
      },
      "source": [
        "import random\n",
        "\n",
        "def sigma (X,Y, mu0, mu1): \n",
        "    m = len(Y)\n",
        "    n = X.shape[1]\n",
        "    sigma = [[0]*n]*n\n",
        "    for i in range(m): \n",
        " \n",
        "        if Y[i] == 0: \n",
        "            temp1 = np.array([X[i]-mu0])\n",
        "            temp2 = np.transpose(temp1)\n",
        "            result = np.dot(temp2, temp1)\n",
        "            sigma = sigma + result\n",
        "        if Y[i] == 1: \n",
        "            temp1 = np.array([X[i]-mu1])\n",
        "            temp2 = np.transpose(temp1)\n",
        "            result = np.dot(temp2, temp1)\n",
        "            sigma = sigma + result\n",
        "    return sigma \n",
        "\n",
        "mu0 = mui(X,Y,0)\n",
        "mu1 = mui(X,Y,1)\n",
        "sigmaa= sigma(X,Y,mu0,mu1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfor i in range(sigmaa.shape[0]): \\n    c = random.random()\\n    print(c)\\n    k = random.randrange(0, sigmaa.shape[0])\\n    sigmaa[i][k] = c\\n\\nfor i in range(sigmaa.shape[0]): \\n    for j in range(i+1, sigmaa.shape[0]): \\n        c = 1\\n        for k in range(sigmaa.shape[1]):\\n            if (sigmaa[i][k] != sigmaa[j][k]):\\n                c = 0\\n        if c ==1: \\n            print(str(i) + ' equals ' + str(j))\\n\\nsigma_inv = np.linalg.pinv(sigmaa)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_mYrta1ZKwp"
      },
      "source": [
        "## predicting values \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "142hF-4dZOfN",
        "outputId": "b61d9b5f-ff84-414d-ee5a-5a9fbd0e3ef5"
      },
      "source": [
        "import math\n",
        "\n",
        "def calculate_px_py(x, mu, sigma):\n",
        "  n = 1\n",
        "  pi = 3.14\n",
        "  dim = len(mu)\n",
        "  \n",
        "\n",
        "  temp1 = np.array([x-mu])\n",
        "  temp2 = np.transpose(temp1)\n",
        "\n",
        "  val =  pow(2*pi,dim/2)\n",
        "  sigma_det = np.sqrt(np.linalg.det(sigma))\n",
        "  sigma_det = 0.1\n",
        "  val = val * sigma_det\n",
        "  val = 1/val\n",
        "  \n",
        "  sigma_inv = np.linalg.pinv(sigma)\n",
        "  multi = np.dot(temp1, sigma_inv)\n",
        "  multi = np.dot(multi, temp2)\n",
        "  return val *  math.exp(-0.5*multi[0][0])\n",
        "\n",
        "\n",
        "mu0 = mui(X,Y,0)\n",
        "mu1 = mui(X,Y,1)\n",
        "sigmaa = sigma(X,Y,mu0,mu1)\n",
        "print('done')\n",
        "print(calculate_px_py(X[0], mu0, sigmaa))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n",
            "1.7424674297567923e-96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX2cY2WOZShs"
      },
      "source": [
        "## getting accuracy ratio of Guassian Discriminant Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVWQQTyGZWZt",
        "outputId": "0f8d47b3-b92d-4dcf-90ce-2eea9b70b302"
      },
      "source": [
        "x1 = X[9]\n",
        "print(Y[9])\n",
        "\n",
        "mu0 = mui(X,Y,0)\n",
        "mu1 = mui(X,Y,1)\n",
        "sigmaa = sigma(X,Y,mu0,mu1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def testing (xtest, ytest, mu0, mu1, sigma):\n",
        "\n",
        "    l = len(xtest)\n",
        "    count = 0 \n",
        "  \n",
        "    for i in range(l):\n",
        "        px0_0 = calculate_px_py(xtest[i], mu0, sigma) * 0.5\n",
        "        px0_1 = calculate_px_py(xtest[i], mu1, sigma) * 0.5\n",
        "        if px0_0 > px0_1: \n",
        "            predicted = 0\n",
        "        else: \n",
        "            predicted = 1\n",
        "          \n",
        "        print('probability of class 0 = ' + str(px0_0) + ', probability of class 1 = ' + str(px0_1) + ', actual class = '+ str(ytest[i]) , 'predicted class = ' + str(predicted))\n",
        "        if predicted == ytest[i]:  \n",
        "            count = count + 1\n",
        "    #print ('accuracy is = ' + str(count/l))\n",
        "    return count/l\n",
        "\n",
        "testing(X, Y, mu0, mu1, sigmaa)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.]\n",
            "probability of class 0 = 8.712337148783961e-97, probability of class 1 = 9.1445473342331e-97, actual class = [0.] predicted class = 1\n",
            "probability of class 0 = 8.712337148783971e-97, probability of class 1 = 9.097220398849903e-97, actual class = [0.] predicted class = 1\n",
            "probability of class 0 = 8.712337148783968e-97, probability of class 1 = 8.753308600228081e-97, actual class = [0.] predicted class = 1\n",
            "probability of class 0 = 8.712337148783968e-97, probability of class 1 = 8.573040886172521e-97, actual class = [0.] predicted class = 0\n",
            "probability of class 0 = 8.712337148783965e-97, probability of class 1 = 7.77177435361734e-97, actual class = [0.] predicted class = 0\n",
            "probability of class 0 = 8.712337148783967e-97, probability of class 1 = 8.67585064050762e-97, actual class = [0.] predicted class = 0\n",
            "probability of class 0 = 8.712337148783968e-97, probability of class 1 = 8.16725791628277e-97, actual class = [0.] predicted class = 0\n",
            "probability of class 0 = 8.712337148783967e-97, probability of class 1 = 8.292613026240312e-97, actual class = [0.] predicted class = 0\n",
            "probability of class 0 = 8.712337148783964e-97, probability of class 1 = 8.707566676488439e-97, actual class = [0.] predicted class = 0\n",
            "probability of class 0 = 8.712337148783967e-97, probability of class 1 = 8.439809928727501e-97, actual class = [0.] predicted class = 0\n",
            "probability of class 0 = 8.712337148783967e-97, probability of class 1 = 8.793736089963978e-97, actual class = [0.] predicted class = 1\n",
            "probability of class 0 = 8.712337148783967e-97, probability of class 1 = 8.500400998834023e-97, actual class = [0.] predicted class = 0\n",
            "probability of class 0 = 8.712337148783968e-97, probability of class 1 = 8.783231257106346e-97, actual class = [0.] predicted class = 1\n",
            "probability of class 0 = 8.712337148783967e-97, probability of class 1 = 8.768594416506646e-97, actual class = [0.] predicted class = 1\n",
            "probability of class 0 = 8.712337148783968e-97, probability of class 1 = 8.395608925779061e-97, actual class = [0.] predicted class = 0\n",
            "probability of class 0 = 8.712337148783967e-97, probability of class 1 = 8.35269310930794e-97, actual class = [0.] predicted class = 0\n",
            "probability of class 0 = 8.71233714878397e-97, probability of class 1 = 8.310994591196934e-97, actual class = [0.] predicted class = 0\n",
            "probability of class 0 = 8.712337148783965e-97, probability of class 1 = 8.4986808731399e-97, actual class = [0.] predicted class = 0\n",
            "probability of class 0 = 8.712337148783968e-97, probability of class 1 = 8.074446791077738e-97, actual class = [0.] predicted class = 0\n",
            "probability of class 0 = 8.385479938870734e-97, probability of class 1 = 8.71233714878397e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.337504970092587e-97, probability of class 1 = 8.712337148783967e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.492328120210762e-97, probability of class 1 = 8.712337148783967e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.286961906084422e-97, probability of class 1 = 8.712337148783968e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.586007956315026e-97, probability of class 1 = 8.712337148783967e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.207558536574854e-97, probability of class 1 = 8.712337148783967e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.206582257808585e-97, probability of class 1 = 8.712337148783971e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.972083541751837e-97, probability of class 1 = 8.712337148783961e-97, actual class = [1.] predicted class = 0\n",
            "probability of class 0 = 8.665388721083163e-97, probability of class 1 = 8.712337148783968e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 9.016290035748579e-97, probability of class 1 = 8.71233714878397e-97, actual class = [1.] predicted class = 0\n",
            "probability of class 0 = 8.594680395649585e-97, probability of class 1 = 8.71233714878397e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.547299636740817e-97, probability of class 1 = 8.712337148783964e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.407123458440607e-97, probability of class 1 = 8.712337148783968e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.995312622843994e-97, probability of class 1 = 8.712337148783968e-97, actual class = [1.] predicted class = 0\n",
            "probability of class 0 = 8.55231609331932e-97, probability of class 1 = 8.71233714878397e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.618258787389651e-97, probability of class 1 = 8.712337148783967e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.424939355142719e-97, probability of class 1 = 8.712337148783967e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.350362707751653e-97, probability of class 1 = 8.712337148783968e-97, actual class = [1.] predicted class = 1\n",
            "probability of class 0 = 8.395371252834344e-97, probability of class 1 = 8.712337148783965e-97, actual class = [1.] predicted class = 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7631578947368421"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}